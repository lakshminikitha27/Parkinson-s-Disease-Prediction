{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "B1Doo0hHKEk5",
      "metadata": {
        "id": "B1Doo0hHKEk5"
      },
      "source": [
        "# Ensembling vgg16-Resnet50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3d197f0",
      "metadata": {
        "id": "b3d197f0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Average, Concatenate\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6620cdab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['normal', 'parkinson']\n"
          ]
        }
      ],
      "source": [
        "# Set dataset folder\n",
        "dataset_folder = os.path.join(os.getcwd(), \"parkinsons_dataset\")\n",
        "folders = os.listdir(dataset_folder)\n",
        "print(folders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b4de541",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% Load images and labels\n",
        "# Define the updated label dictionary\n",
        "label_dict = {\n",
        "    'normal': 0,\n",
        "    'parkinson': 1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dfcb2ca8",
      "metadata": {
        "id": "dfcb2ca8"
      },
      "outputs": [],
      "source": [
        "# Function to load images from folder with reduced target size for memory optimization\n",
        "def load_images_from_folder(folder_path, label_dict, target_size=(224, 224)):  # Changed target size to 224x224\n",
        "    image_data = []\n",
        "    labels = []\n",
        "    folders = os.listdir(folder_path)\n",
        "    \n",
        "    for folder in folders:\n",
        "        path = os.path.join(folder_path, folder)\n",
        "        for im in os.listdir(path):\n",
        "            img = image.load_img(os.path.join(path, im), target_size=target_size)\n",
        "            img_array = image.img_to_array(img)\n",
        "            image_data.append(img_array)\n",
        "            labels.append(label_dict[folder])    \n",
        "    return np.array(image_data), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d20c7dcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20c7dcf",
        "outputId": "92d61dbc-e0b2-4f7a-ea33-adfb527974c1"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "x_data, y_data = load_images_from_folder(dataset_folder, label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1b9e8e10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% Split the data into training (80%) and testing (20%) sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a6555eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6555eb",
        "outputId": "1f252011-f31c-45ec-e20e-531eeb3ab724"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zx919sbKNFFB",
      "metadata": {
        "id": "zx919sbKNFFB"
      },
      "source": [
        "**Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Z_JTSbpNdtFW",
      "metadata": {
        "id": "Z_JTSbpNdtFW"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for training\n",
        "augment = ImageDataGenerator(rotation_range=20,\n",
        "                             width_shift_range=0.01,\n",
        "                             height_shift_range=0.01,\n",
        "                             horizontal_flip=False,\n",
        "                             vertical_flip=False)\n",
        "augment.fit(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ad27c22e",
      "metadata": {
        "id": "ad27c22e"
      },
      "outputs": [],
      "source": [
        "# %% Model 1: VGG16\n",
        "# Load the VGG16 model and freeze more layers\n",
        "vgg16_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))  # Adjusted input shape\n",
        "for layer in vgg16_model.layers[:12]:  # Freeze more layers to optimize memory\n",
        "    layer.trainable = False\n",
        "\n",
        "vgg16_output = Flatten()(vgg16_model.output)\n",
        "vgg16_fc1 = Dense(256, activation='relu')(vgg16_output)\n",
        "vgg16_dropout1 = Dropout(0.5)(vgg16_fc1)\n",
        "vgg16_fc2 = Dense(128, activation='relu')(vgg16_dropout1)\n",
        "vgg16_dropout2 = Dropout(0.5)(vgg16_fc2)\n",
        "vgg16_output_layer = Dense(2, activation='sigmoid')(vgg16_dropout2)\n",
        "model_vgg = Model(vgg16_model.input, vgg16_output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f82e266c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% Model 2: ResNet50\n",
        "# Load the ResNet50 model and freeze more layers\n",
        "resnet50_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))  # Adjusted input shape\n",
        "for layer in resnet50_model.layers[:150]:  # Freeze more layers to optimize memory\n",
        "    layer.trainable = False\n",
        "\n",
        "resnet_output = Flatten()(resnet50_model.output)\n",
        "resnet_fc1 = Dense(256, activation='relu')(resnet_output)\n",
        "resnet_dropout1 = Dropout(0.5)(resnet_fc1)\n",
        "resnet_fc2 = Dense(128, activation='relu')(resnet_dropout1)\n",
        "resnet_dropout2 = Dropout(0.5)(resnet_fc2)\n",
        "resnet_output_layer = Dense(2, activation='sigmoid')(resnet_dropout2)\n",
        "model_resnet = Model(resnet50_model.input, resnet_output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb0930ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% Ensemble the models by concatenating their outputs for better performance\n",
        "model_input = tf.keras.Input(shape=(224, 224, 3))  # Adjusted input shape\n",
        "vgg_output = model_vgg(model_input)\n",
        "resnet_output = model_resnet(model_input)\n",
        "ensemble_output = Concatenate()([vgg_output, resnet_output])  # Concatenate instead of average for more information\n",
        "final_output = Dense(2, activation='sigmoid')(ensemble_output)  # Final output layer\n",
        "ensemble_model = Model(inputs=model_input, outputs=final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "aacbada1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% Compile the ensemble model\n",
        "adam = Adam(learning_rate=0.0001)\n",
        "ensemble_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dd6d25df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model checkpoint\n",
        "checkpoint_filepath = \"parkinsons_detection_ensemble.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min', patience=5)\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "206aa399",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7462 - accuracy: 0.6750 \n",
            "Epoch 1: val_loss improved from inf to 0.44271, saving model to parkinsons_detection_ensemble.hdf5\n",
            "10/10 [==============================] - 671s 68s/step - loss: 0.7462 - accuracy: 0.6750 - val_loss: 0.4427 - val_accuracy: 0.9250\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.7375  \n",
            "Epoch 2: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 677s 70s/step - loss: 0.6816 - accuracy: 0.7375 - val_loss: 0.5364 - val_accuracy: 0.9500\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6151 - accuracy: 0.8500 \n",
            "Epoch 3: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 667s 68s/step - loss: 0.6151 - accuracy: 0.8500 - val_loss: 0.5617 - val_accuracy: 0.9250\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.8625  \n",
            "Epoch 4: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 744s 76s/step - loss: 0.6173 - accuracy: 0.8625 - val_loss: 0.5603 - val_accuracy: 0.9250\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.8438 \n",
            "Epoch 5: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 584s 57s/step - loss: 0.6343 - accuracy: 0.8438 - val_loss: 0.5351 - val_accuracy: 0.9750\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.6092 - accuracy: 0.8687 \n",
            "Epoch 6: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 643s 67s/step - loss: 0.6092 - accuracy: 0.8687 - val_loss: 0.5370 - val_accuracy: 0.9750\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.8813 \n",
            "Epoch 7: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 590s 60s/step - loss: 0.5810 - accuracy: 0.8813 - val_loss: 0.5352 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7334 - accuracy: 0.7063 \n",
            "Epoch 8: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 635s 65s/step - loss: 0.7334 - accuracy: 0.7063 - val_loss: 0.8765 - val_accuracy: 0.5250\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.7063 \n",
            "Epoch 9: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 636s 65s/step - loss: 0.7148 - accuracy: 0.7063 - val_loss: 0.8736 - val_accuracy: 0.5250\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.6125 \n",
            "Epoch 10: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 635s 65s/step - loss: 0.8231 - accuracy: 0.6125 - val_loss: 0.8688 - val_accuracy: 0.5250\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7890 - accuracy: 0.6375 \n",
            "Epoch 11: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 539s 55s/step - loss: 0.7890 - accuracy: 0.6375 - val_loss: 0.8661 - val_accuracy: 0.5250\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7849 - accuracy: 0.6625 \n",
            "Epoch 12: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 515s 52s/step - loss: 0.7849 - accuracy: 0.6625 - val_loss: 0.8634 - val_accuracy: 0.5250\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8230 - accuracy: 0.6000 \n",
            "Epoch 13: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 514s 52s/step - loss: 0.8230 - accuracy: 0.6000 - val_loss: 0.8608 - val_accuracy: 0.5250\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.6187 \n",
            "Epoch 14: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 513s 52s/step - loss: 0.7833 - accuracy: 0.6187 - val_loss: 0.8581 - val_accuracy: 0.5250\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8010 - accuracy: 0.6000 \n",
            "Epoch 15: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 515s 52s/step - loss: 0.8010 - accuracy: 0.6000 - val_loss: 0.8555 - val_accuracy: 0.5250\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7931 - accuracy: 0.6062 \n",
            "Epoch 16: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 511s 52s/step - loss: 0.7931 - accuracy: 0.6062 - val_loss: 0.8529 - val_accuracy: 0.5250\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.6313 \n",
            "Epoch 17: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 499s 51s/step - loss: 0.7531 - accuracy: 0.6313 - val_loss: 0.8527 - val_accuracy: 0.5250\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7746 - accuracy: 0.6062 \n",
            "Epoch 18: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 507s 52s/step - loss: 0.7746 - accuracy: 0.6062 - val_loss: 0.8506 - val_accuracy: 0.5250\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.5813 \n",
            "Epoch 19: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 510s 52s/step - loss: 0.7929 - accuracy: 0.5813 - val_loss: 0.8481 - val_accuracy: 0.5250\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.6000 \n",
            "Epoch 20: val_loss did not improve from 0.44271\n",
            "10/10 [==============================] - 513s 52s/step - loss: 0.7732 - accuracy: 0.6000 - val_loss: 0.8429 - val_accuracy: 0.5250\n"
          ]
        }
      ],
      "source": [
        "# %% Train the model with reduced batch size\n",
        "hist = ensemble_model.fit(augment.flow(x_train, y_train, batch_size=16),  # Reduced batch size to 16\n",
        "                          epochs=20,\n",
        "                          validation_data=(x_test, y_test),\n",
        "                          callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cfa1c98e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best saved model\n",
        "best_model = load_model(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "31ad7a43",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 62s 14s/step - loss: 0.4427 - accuracy: 0.9250\n",
            "Test Loss: 0.44271135330200195, Test Accuracy: 0.925000011920929\n"
          ]
        }
      ],
      "source": [
        "# %% Evaluate the model on the test data\n",
        "test_loss, test_accuracy = best_model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "551c318f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 71s 15s/step\n"
          ]
        }
      ],
      "source": [
        "# %% Predict test data\n",
        "predictions = best_model.predict(x_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8df7c551",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.925\n",
            "Precision: 0.875\n",
            "Recall: 1.0\n",
            "F1-score: 0.9333333333333333\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.84      0.91        19\n",
            "           1       0.88      1.00      0.93        21\n",
            "\n",
            "    accuracy                           0.93        40\n",
            "   macro avg       0.94      0.92      0.92        40\n",
            "weighted avg       0.93      0.93      0.92        40\n",
            "\n",
            "Confusion Matrix:\n",
            "[[16  3]\n",
            " [ 0 21]]\n"
          ]
        }
      ],
      "source": [
        "# %% Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predicted_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VGG-Resnet-Ensembel_Deep_parkinson's.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
